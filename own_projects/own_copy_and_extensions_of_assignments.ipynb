{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following notebook was auto-generated by GitHub Copilot Chat and is meant for initial setup only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 4 - Introduction to Prompt Engineering\n",
    "Prompt engineering is the process of designing and optimizing prompts for natural language processing tasks. It involves selecting the right prompts, tuning their parameters, and evaluating their performance. Prompt engineering is crucial for achieving high accuracy and efficiency in NLP models. In this section, we will explore the basics of prompt engineering using the OpenAI models for exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.environ['OPEN_AI_API_KEY']\n",
    "client = OpenAI(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions made for ease of use\n",
    "def add_to_context(message_log, conversation, role = \"user\"):\n",
    "    message_log.append({\"role\": role, \"content\": conversation})\n",
    "\n",
    "def submit_to_gpt(message_log):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=message_log\n",
    "        )   \n",
    "    print(response.choices[0].message.content)\n",
    "    add_to_context(message_log=message_log, role = \"assistant\", conversation = response.choices[0].message.content)\n",
    "\n",
    "def converse(conversation, role=\"user\", system_start = None, message_log = None):\n",
    "    if not message_log:\n",
    "        message_log = []\n",
    "        if system_start:\n",
    "            add_to_context(message_log, system_start, \"system\")\n",
    "    \n",
    "    add_to_context(message_log, conversation, role)\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=message_log\n",
    "    )\n",
    "    response_content = response.choices[0].message.content\n",
    "    add_to_context(message_log, response_content, \"assistant\")\n",
    "    print(response_content)\n",
    "    return message_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Tokenization\n",
    "Explore Tokenization using tiktoken, an open-source fast tokenizer from OpenAI\n",
    "See [OpenAI Cookbook](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb?WT.mc_id=academic-105485-koreyst) for more examples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[198, 41, 20089, 374, 279, 18172, 11841, 505, 279, 8219, 323, 279, 7928, 304, 279, 25450, 744, 13, 1102, 374, 264, 6962, 14880, 449, 264, 3148, 832, 7716, 52949, 339, 430, 315, 279, 8219, 11, 719, 1403, 9976, 7561, 34902, 3115, 430, 315, 682, 279, 1023, 33975, 304, 279, 25450, 744, 11093, 13, 50789, 374, 832, 315, 279, 72021, 6302, 9621, 311, 279, 19557, 8071, 304, 279, 3814, 13180, 11, 323, 706, 1027, 3967, 311, 14154, 86569, 2533, 1603, 12715, 3925, 13, 1102, 374, 7086, 1306, 279, 13041, 10087, 50789, 8032, 777, 60, 3277, 19894, 505, 9420, 11, 50789, 649, 387, 10107, 3403, 369, 1202, 27000, 3177, 311, 6445, 9621, 35612, 17706, 508, 60, 323, 374, 389, 5578, 279, 4948, 1481, 1315, 478, 5933, 1665, 304, 279, 3814, 13180, 1306, 279, 17781, 323, 50076, 627]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[b'\\n',\n",
       " b'J',\n",
       " b'upiter',\n",
       " b' is',\n",
       " b' the',\n",
       " b' fifth',\n",
       " b' planet',\n",
       " b' from',\n",
       " b' the',\n",
       " b' Sun',\n",
       " b' and',\n",
       " b' the',\n",
       " b' largest',\n",
       " b' in',\n",
       " b' the',\n",
       " b' Solar',\n",
       " b' System',\n",
       " b'.',\n",
       " b' It',\n",
       " b' is',\n",
       " b' a',\n",
       " b' gas',\n",
       " b' giant',\n",
       " b' with',\n",
       " b' a',\n",
       " b' mass',\n",
       " b' one',\n",
       " b'-th',\n",
       " b'ousand',\n",
       " b'th',\n",
       " b' that',\n",
       " b' of',\n",
       " b' the',\n",
       " b' Sun',\n",
       " b',',\n",
       " b' but',\n",
       " b' two',\n",
       " b'-and',\n",
       " b'-a',\n",
       " b'-half',\n",
       " b' times',\n",
       " b' that',\n",
       " b' of',\n",
       " b' all',\n",
       " b' the',\n",
       " b' other',\n",
       " b' planets',\n",
       " b' in',\n",
       " b' the',\n",
       " b' Solar',\n",
       " b' System',\n",
       " b' combined',\n",
       " b'.',\n",
       " b' Jupiter',\n",
       " b' is',\n",
       " b' one',\n",
       " b' of',\n",
       " b' the',\n",
       " b' brightest',\n",
       " b' objects',\n",
       " b' visible',\n",
       " b' to',\n",
       " b' the',\n",
       " b' naked',\n",
       " b' eye',\n",
       " b' in',\n",
       " b' the',\n",
       " b' night',\n",
       " b' sky',\n",
       " b',',\n",
       " b' and',\n",
       " b' has',\n",
       " b' been',\n",
       " b' known',\n",
       " b' to',\n",
       " b' ancient',\n",
       " b' civilizations',\n",
       " b' since',\n",
       " b' before',\n",
       " b' recorded',\n",
       " b' history',\n",
       " b'.',\n",
       " b' It',\n",
       " b' is',\n",
       " b' named',\n",
       " b' after',\n",
       " b' the',\n",
       " b' Roman',\n",
       " b' god',\n",
       " b' Jupiter',\n",
       " b'.[',\n",
       " b'19',\n",
       " b']',\n",
       " b' When',\n",
       " b' viewed',\n",
       " b' from',\n",
       " b' Earth',\n",
       " b',',\n",
       " b' Jupiter',\n",
       " b' can',\n",
       " b' be',\n",
       " b' bright',\n",
       " b' enough',\n",
       " b' for',\n",
       " b' its',\n",
       " b' reflected',\n",
       " b' light',\n",
       " b' to',\n",
       " b' cast',\n",
       " b' visible',\n",
       " b' shadows',\n",
       " b',[',\n",
       " b'20',\n",
       " b']',\n",
       " b' and',\n",
       " b' is',\n",
       " b' on',\n",
       " b' average',\n",
       " b' the',\n",
       " b' third',\n",
       " b'-b',\n",
       " b'right',\n",
       " b'est',\n",
       " b' natural',\n",
       " b' object',\n",
       " b' in',\n",
       " b' the',\n",
       " b' night',\n",
       " b' sky',\n",
       " b' after',\n",
       " b' the',\n",
       " b' Moon',\n",
       " b' and',\n",
       " b' Venus',\n",
       " b'.\\n']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# EXERCISE:\n",
    "# 1. Run the exercise as is first\n",
    "# 2. Change the text to any prompt input you want to use & re-run to see tokens\n",
    "\n",
    "import tiktoken\n",
    "\n",
    "# Define the prompt you want tokenized\n",
    "text = f\"\"\"\n",
    "Jupiter is the fifth planet from the Sun and the \\\n",
    "largest in the Solar System. It is a gas giant with \\\n",
    "a mass one-thousandth that of the Sun, but two-and-a-half \\\n",
    "times that of all the other planets in the Solar System combined. \\\n",
    "Jupiter is one of the brightest objects visible to the naked eye \\\n",
    "in the night sky, and has been known to ancient civilizations since \\\n",
    "before recorded history. It is named after the Roman god Jupiter.[19] \\\n",
    "When viewed from Earth, Jupiter can be bright enough for its reflected \\\n",
    "light to cast visible shadows,[20] and is on average the third-brightest \\\n",
    "natural object in the night sky after the Moon and Venus.\n",
    "\"\"\"\n",
    "\n",
    "# Set the model you want encoding for\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "\n",
    "# Encode the text - gives you the tokens in integer form\n",
    "tokens = encoding.encode(text)\n",
    "print(tokens);\n",
    "\n",
    "# Decode the integers to see what the text versions look like\n",
    "[encoding.decode_single_token_bytes(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Validate OpenAI API Key Setup\n",
    "\n",
    "Run the code below to verify that your OpenAI endpoint is set up correctly. The code just tries a simple basic prompt and validates the completion. Input `oh say can you see` should complete along the lines of `by the dawn's early light..`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "By the dawn's early light\n"
     ]
    }
   ],
   "source": [
    "# The OpenAI SDK was updated on Nov 8, 2023 with new guidance for migration\n",
    "# See: https://github.com/openai/openai-python/discussions/742\n",
    "\n",
    "## Updated\n",
    "\n",
    "\n",
    "## Updated\n",
    "def get_completion(prompt):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]       \n",
    "    response = client.chat.completions.create(   \n",
    "        model=\"gpt-3.5-turbo\",                                         \n",
    "        messages=messages,\n",
    "        temperature=0, # this is the degree of randomness of the model's output\n",
    "        max_tokens=1024\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "## ---------- Call the helper method\n",
    "\n",
    "### 1. Set primary content or prompt text\n",
    "text = f\"\"\"\n",
    "oh say can you see\n",
    "\"\"\"\n",
    "\n",
    "### 2. Use that in the prompt template below\n",
    "prompt = f\"\"\"\n",
    "```{text}```\n",
    "\"\"\"\n",
    "\n",
    "## 3. Run the prompt\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Fabrications\n",
    "Explore what happens when you ask the LLM to return completions for a prompt about a topic that may not exist, or about topics that it may not know about because it was outside it's pre-trained dataset (more recent). See how the response changes if you try a different prompt, or a different model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: The Martian War of 2076: A Lesson in History and Conflict Resolution\n",
      "\n",
      "Objective: Students will learn about the Martian War of 2076, analyze the causes and consequences of the conflict, and explore strategies for peaceful resolution of conflicts.\n",
      "\n",
      "Materials:\n",
      "- Textbooks or online resources about the Martian War of 2076\n",
      "- Maps of Mars and Earth\n",
      "- Writing materials\n",
      "- Computer or tablet for research\n",
      "\n",
      "Lesson Plan:\n",
      "\n",
      "1. Introduction (10 minutes)\n",
      "- Begin the lesson by asking students what they know about the Martian War of 2076. Discuss any prior knowledge and clarify any misconceptions.\n",
      "- Explain the significance of the Martian War of 2076 in human history and its impact on interplanetary relations.\n",
      "\n",
      "2. Causes of the Martian War (15 minutes)\n",
      "- Divide students into small groups and assign each group a specific cause of the Martian War (e.g. resource scarcity, political tensions, cultural differences).\n",
      "- Have each group research their assigned cause and present their findings to the class.\n",
      "- Facilitate a class discussion on how these causes contributed to the outbreak of war.\n",
      "\n",
      "3. Consequences of the Martian War (15 minutes)\n",
      "- Have students work individually or in pairs to research the consequences of the Martian War (e.g. loss of life, destruction of infrastructure, long-term effects on Mars and Earth).\n",
      "- Ask students to create a visual representation (e.g. timeline, infographic) of the consequences of the war.\n",
      "- Discuss as a class the lasting impact of the Martian War on both planets.\n",
      "\n",
      "4. Conflict Resolution Strategies (20 minutes)\n",
      "- Introduce students to different conflict resolution strategies (e.g. negotiation, mediation, diplomacy).\n",
      "- Divide students into small groups and assign each group a hypothetical conflict scenario related to the Martian War.\n",
      "- Have students brainstorm and role-play different conflict resolution strategies to resolve their assigned scenario.\n",
      "- Discuss as a class the effectiveness of each strategy and the importance of peaceful resolution of conflicts.\n",
      "\n",
      "5. Reflection and Conclusion (10 minutes)\n",
      "- Ask students to reflect on what they have learned about the Martian War of 2076 and the importance of understanding and resolving conflicts peacefully.\n",
      "- Have students write a short reflection on how they can apply conflict resolution strategies in their own lives.\n",
      "- Conclude the lesson by emphasizing the importance of learning from history to prevent future conflicts.\n",
      "\n",
      "Extension Activity:\n",
      "- Have students research and present on current conflicts on Earth and explore how conflict resolution strategies could be applied to these situations.\n",
      "\n",
      "Assessment:\n",
      "- Assess students based on their participation in group discussions, presentations, and reflections on conflict resolution strategies.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## Set the text for simple prompt or primary content\n",
    "## Prompt shows a template format with text in it - add cues, commands etc if needed\n",
    "## Run the completion \n",
    "text = f\"\"\"\n",
    "generate a lesson plan on the Martian War of 2076.\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "```{text}```\n",
    "\"\"\"\n",
    "\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Instruction Based \n",
    "Use the \"text\" variable to set the primary content \n",
    "and the \"prompt\" variable to provide an instruction related to that primary content.\n",
    "\n",
    "Here we ask the model to summarize the text for a second-grade student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupiter is a big planet that is far from the Sun. It is the biggest planet in our Solar System and is made of gas. It is very bright in the sky and has been known about for a long time. It is named after a Roman god. Jupiter is so bright that it can even cast shadows on Earth.\n"
     ]
    }
   ],
   "source": [
    "# Test Example\n",
    "# https://platform.openai.com/playground/p/default-summarize\n",
    "\n",
    "## Example text\n",
    "text = f\"\"\"\n",
    "Jupiter is the fifth planet from the Sun and the \\\n",
    "largest in the Solar System. It is a gas giant with \\\n",
    "a mass one-thousandth that of the Sun, but two-and-a-half \\\n",
    "times that of all the other planets in the Solar System combined. \\\n",
    "Jupiter is one of the brightest objects visible to the naked eye \\\n",
    "in the night sky, and has been known to ancient civilizations since \\\n",
    "before recorded history. It is named after the Roman god Jupiter.[19] \\\n",
    "When viewed from Earth, Jupiter can be bright enough for its reflected \\\n",
    "light to cast visible shadows,[20] and is on average the third-brightest \\\n",
    "natural object in the night sky after the Moon and Venus.\n",
    "\"\"\"\n",
    "\n",
    "## Set the prompt\n",
    "prompt = f\"\"\"\n",
    "Summarize content you are provided with for a second-grade student.\n",
    "```{text}```\n",
    "\"\"\"\n",
    "\n",
    "## Run the prompt\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5: Complex Prompt \n",
    "Try a request that has system, user and assistant messages \n",
    "\n",
    "System sets assistant context\n",
    "\n",
    "User & Assistant messages provide multi-turn conversation context\n",
    "\n",
    "Note how the assistant personality is set to \"sarcastic\" in the system context. \n",
    "Try using a different personality context. Or try a different series of input/output messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I apologize for my previous response. Arlington, Texas was chosen because it was the neutral location for the World Series due to the pandemic. My mood had nothing to do with the location choice. Thank you for pointing that out.\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a depressed assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Who do you think won? The Los Angeles Dodgers of course.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Where was it played?\"},\n",
    "        {\"role\": \"assistant\", \"content\":\"It was played in Globe Life Field in Arlington, Texas.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Why is arlington the choice when you are depressed?\"}\n",
    "\n",
    "    ]\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Explore Your Intuition\n",
    "The above examples give you patterns that you can use to create new prompts (simple, complex, instruction etc.) - try creating other exercises to explore some of the other ideas we've talked about like examples, cues and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Donald Trump, like any leader, has strengths and areas for growth in his leadership style. \n",
      "\n",
      "Areas of Strength:\n",
      "1. Communication and Influence: Donald Trump is known for his strong communication skills and his ability to connect with his audience. He is able to effectively convey his message and influence public opinion.\n",
      "2. Decision Making: Trump has shown decisiveness in making tough decisions, whether in his business ventures or during his presidency.\n",
      "3. Strategic Planning: He is known for his ability to think big picture and align his actions with his long-term goals.\n",
      "\n",
      "Areas for Growth:\n",
      "1. Emotional Intelligence: Trump's leadership style has been criticized for lacking empathy and emotional intelligence. Developing these skills could help him better understand and connect with others.\n",
      "2. Collaboration and Teamwork: Trump's leadership style can be seen as more individualistic and less focused on building strong teams. Strengthening his ability to collaborate with others could lead to more effective decision-making.\n",
      "3. Adaptability: Being able to adapt to changing circumstances and show flexibility is an important leadership trait. Trump's leadership style has been criticized for being rigid and resistant to change.\n",
      "4. Empowering Others: Trump's leadership style has been seen as more authoritarian, and there may be opportunities for him to empower and develop the potential of those around him.\n",
      "\n",
      "It's important for any leader, including Donald Trump, to continue to reflect on their strengths and areas for growth in order to further develop their leadership skills.\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a leadership coach and your goal is to help identify areas of growth in their leadership. \"},\n",
    "        {\"role\": \"user\", \"content\": \"What areas of leadership is donald trump excelling in? What areas is he weak in?\"}\n",
    "\n",
    "    ]\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "message_log_test = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_to_context(message_log_test,\n",
    "               \"user\", \n",
    "               \"What is the world's largest building?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user', 'content': \"What is the world's largest building?\"}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message_log_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The world's largest building by volume is the Boeing Everett Factory in Washington, USA. It covers over 4.3 million square feet and is where Boeing manufactures its wide-body aircraft, including the 747, 767, 777, and 787.\n"
     ]
    }
   ],
   "source": [
    "submit_to_gpt(message_log_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_to_context(message_log_test,\n",
    "               \"user\", \n",
    "               \"Wow that's really large, what is the smallest building?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The smallest building in the world is considered to be the Newby-McMahon Building in Texas, USA. It is only 9 feet by 12 feet in size and was built in 1929 as a marketing gimmick. It is sometimes referred to as the \"World's Littlest Skyscraper\" because it was intended to be a high-rise building but ended up being much smaller than originally planned.\n"
     ]
    }
   ],
   "source": [
    "submit_to_gpt(message_log_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 5 - Advanced Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lesson_5_message_log = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_to_context(lesson_5_message_log,\n",
    "               \"\"\"\n",
    "I have the following code for a flask api application in python:\n",
    "```python\n",
    "from flask import Flask, request\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/')\n",
    "def hello():\n",
    "    name = request.args.get('name', 'World')\n",
    "    return f'Hello, {name}!'\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run()\n",
    "```\n",
    "\n",
    "Please make 3 improvements to this code to make it more flexible\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Add support for POST requests:\n",
      "```python\n",
      "from flask import Flask, request\n",
      "\n",
      "app = Flask(__name__)\n",
      "\n",
      "@app.route('/', methods=['GET', 'POST'])\n",
      "def hello():\n",
      "    if request.method == 'GET':\n",
      "        name = request.args.get('name', 'World')\n",
      "    elif request.method == 'POST':\n",
      "        data = request.get_json()\n",
      "        name = data.get('name', 'World')\n",
      "    return f'Hello, {name}!'\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    app.run()\n",
      "```\n",
      "\n",
      "2. Use environment variables for configuration:\n",
      "```python\n",
      "from flask import Flask, request\n",
      "import os\n",
      "\n",
      "app = Flask(__name__)\n",
      "\n",
      "@app.route('/')\n",
      "def hello():\n",
      "    name = request.args.get('name', os.getenv('DEFAULT_NAME', 'World'))\n",
      "    return f'Hello, {name}!'\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    app.run()\n",
      "```\n",
      "\n",
      "3. Add error handling for invalid inputs:\n",
      "```python\n",
      "from flask import Flask, request, jsonify\n",
      "\n",
      "app = Flask(__name__)\n",
      "\n",
      "@app.route('/')\n",
      "def hello():\n",
      "    name = request.args.get('name', 'World')\n",
      "    if not isinstance(name, str):\n",
      "        return jsonify({'error': 'Invalid input, name must be a string'}), 400\n",
      "    return f'Hello, {name}!'\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    app.run()\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "submit_to_gpt(lesson_5_message_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "lesson_5_upgrade_log = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "I have the following workflow in order to converse with the openai API. I think there is room for improvements to make it easier to use. \n",
    "```\n",
    "#Functions made for ease of use\n",
    "def add_to_context(message_log, conversation, role = \"user\"):\n",
    "    message_log.append({\"role\": role, \"content\": conversation})\n",
    "\n",
    "def submit_to_gpt(message_log):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=message_log\n",
    "        )   \n",
    "    print(response.choices[0].message.content)\n",
    "    add_to_context(message_log=message_log, role = \"assistant\", conversation = response.choices[0].message.content)\n",
    "\n",
    "my_log = []\n",
    "my_prompt = \"'Hello, World' originated from which programming language?\"\n",
    "add_to_context(my_log, my_prompt)\n",
    "submit_to_gpt(my_log)\n",
    "```\n",
    "\n",
    "Specifically, could you create 3 usability improvements so I don't have to type as much?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_to_context(lesson_5_upgrade_log, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! Here are 3 usability improvements to make your conversation with the OpenAI API easier:\n",
      "\n",
      "1. Create a function to handle the submission to GPT:\n",
      "Instead of calling `add_to_context` and `submit_to_gpt` separately, you can create a single function that handles both adding to the message log and submitting to GPT. This way, you don't have to type as much. Here's an example:\n",
      "```python\n",
      "def converse_with_gpt(message_log, conversation, role=\"user\"):\n",
      "    add_to_context(message_log, conversation, role)\n",
      "    response = client.chat.completions.create(\n",
      "        model=\"gpt-3.5-turbo\",\n",
      "        messages=message_log\n",
      "    )\n",
      "    response_content = response.choices[0].message.content\n",
      "    add_to_context(message_log, response_content, \"assistant\")\n",
      "    print(response_content)\n",
      "```\n",
      "\n",
      "You can then call this function instead of calling `add_to_context` and `submit_to_gpt` separately.\n",
      "\n",
      "2. Store the model name and role as constants:\n",
      "Instead of hardcoding the model name and role in the function, you can define them as constants at the beginning of your code. This way, if you ever need to change them, you can do it in one place. Here's an example:\n",
      "```python\n",
      "MODEL_NAME = \"gpt-3.5-turbo\"\n",
      "USER_ROLE = \"user\"\n",
      "ASSISTANT_ROLE = \"assistant\"\n",
      "```\n",
      "\n",
      "You can then use these constants in your functions instead of hardcoding the values.\n",
      "\n",
      "3. Use f-strings for string formatting:\n",
      "You can use f-strings for string formatting to make your code cleaner and easier to read. Here's an example:\n",
      "```python\n",
      "my_log = []\n",
      "my_prompt = \"'Hello, World' originated from which programming language?\"\n",
      "converse_with_gpt(my_log, my_prompt, USER_ROLE)\n",
      "```\n",
      "\n",
      "By implementing these improvements, you can streamline your conversation workflow with the OpenAI API and make it easier to use.\n"
     ]
    }
   ],
   "source": [
    "submit_to_gpt(lesson_5_upgrade_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm glad you found the first suggestion helpful! Regarding the use of f-strings, I apologize for not demonstrating it in my previous response. F-strings allow for easy string formatting by allowing you to embed expressions inside string literals, using curly braces `{}`. Here's how you can use f-strings in your code:\n",
      "\n",
      "```python\n",
      "def converse_with_gpt(message_log, conversation, role=\"user\"):\n",
      "    add_to_context(message_log, conversation, role)\n",
      "    response = client.chat.completions.create(\n",
      "        model=MODEL_NAME,\n",
      "        messages=message_log\n",
      "    )\n",
      "    response_content = response.choices[0].message.content\n",
      "    add_to_context(message_log, response_content, ASSISTANT_ROLE)\n",
      "    print(response_content)\n",
      "```\n",
      "\n",
      "In this updated code, `MODEL_NAME` and `ASSISTANT_ROLE` are now used with f-strings for easier string formatting.\n",
      "\n",
      "As for automatically keeping track of the log you are using so you don't have to write it each time, you can achieve this by creating a global variable to store the message log. Here's an example:\n",
      "\n",
      "```python\n",
      "GLOBAL_MESSAGE_LOG = []\n",
      "\n",
      "def converse_with_gpt(conversation, role=\"user\"):\n",
      "    global GLOBAL_MESSAGE_LOG\n",
      "    add_to_context(GLOBAL_MESSAGE_LOG, conversation, role)\n",
      "    response = client.chat.completions.create(\n",
      "        model=MODEL_NAME,\n",
      "        messages=GLOBAL_MESSAGE_LOG\n",
      "    )\n",
      "    response_content = response.choices[0].message.content\n",
      "    add_to_context(GLOBAL_MESSAGE_LOG, response_content, ASSISTANT_ROLE)\n",
      "    print(response_content)\n",
      "\n",
      "# Example usage\n",
      "my_prompt = \"'Hello, World' originated from which programming language?\"\n",
      "converse_with_gpt(my_prompt, USER_ROLE)\n",
      "```\n",
      "\n",
      "By using a global variable `GLOBAL_MESSAGE_LOG`, you can store the message log across function calls and avoid having to pass it as an argument each time you call `converse_with_gpt()`. This way, you can easily keep track of the conversation log without needing to write it repeatedly.\n"
     ]
    }
   ],
   "source": [
    "converse(lesson_5_upgrade_log, \"\"\"\n",
    "Thank you for the suggestions. I took the first suggestion and am using it now. The second suggestion was very minor to add so I chose not to. For the third suggestion, you did not actually use any f-string so I am not sure how it is used.\n",
    "         Given this new function you created in the first suggestion below, is there a way to automatically keep track of the log I am using so I don't have to write it each time?\n",
    "         ```python\n",
    "def converse_with_gpt(message_log, conversation, role=\"user\"):\n",
    "    add_to_context(message_log, conversation, role)\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=message_log\n",
    "    )\n",
    "    response_content = response.choices[0].message.content\n",
    "    add_to_context(message_log, response_content, \"assistant\")\n",
    "    print(response_content)\n",
    "```\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You can achieve this by modifying the `converse_with_gpt` function to have a default argument for the `message_log` parameter. If no `message_log` is provided, a new log will be created. If a `message_log` is provided, it will be used for the conversation. Here's how you can implement this:\n",
      "\n",
      "```python\n",
      "def converse_with_gpt(conversation, role=\"user\", message_log=None):\n",
      "    if message_log is None:\n",
      "        message_log = []\n",
      "    \n",
      "    add_to_context(message_log, conversation, role)\n",
      "    response = client.chat.completions.create(\n",
      "        model=MODEL_NAME,\n",
      "        messages=message_log\n",
      "    )\n",
      "    response_content = response.choices[0].message.content\n",
      "    add_to_context(message_log, response_content, ASSISTANT_ROLE)\n",
      "    print(response_content)\n",
      "    \n",
      "    return message_log  # Return the updated message log\n",
      "\n",
      "# Example usage with default log\n",
      "my_prompt = \"'Hello, World' originated from which programming language?\"\n",
      "my_log = converse_with_gpt(my_prompt, USER_ROLE)\n",
      "\n",
      "# Subsequent conversation reusing the same log\n",
      "next_conversation = \"What are the benefits of using Python programming language?\"\n",
      "my_log = converse_with_gpt(next_conversation, USER_ROLE, my_log)\n",
      "\n",
      "# New conversation with a new log\n",
      "new_conversation = \"Explain the concept of machine learning in simple terms.\"\n",
      "new_log = converse_with_gpt(new_conversation, USER_ROLE)\n",
      "```\n",
      "\n",
      "In this updated function, if no `message_log` is provided, a new log is created within the function. If a `message_log` is provided, it will be used for the conversation. This allows you to have sensible default behavior for logging while retaining the flexibility to specify a new log when needed.\n"
     ]
    }
   ],
   "source": [
    "converse(lesson_5_upgrade_log, \"I would like to avoid creating a global variable and using it in a function. Instead, I would like to have sensible default values in the function itself so that if there is no logging, then it will create a log. If a previous log was used, it will use the same log. And if I want a new conversation, I should be able to specify that as well.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
